steps:
  # - label: "DeepSeek"
  #   gpu: h200
  #   num_gpus: 4
  #   source_file_dependencies:
  #     - vllm/
  #   command: CUDA_VISIBLE_DEVICES=1,2,3,4 VLLM_ALL2ALL_BACKEND=pplx VLLM_USE_DEEP_GEMM=1 vllm serve deepseek-ai/DeepSeek-V3-0324 --tensor-parallel-size 2 --data-parallel-size 2 --enable-expert-parallel --max-model-len 1024  --trust-remote-code

  - label: "Llama EP"
    gpu: h200
    num_gpus: 4
    commands:
      - CUDA_VISIBLE_DEVICES=1,2,3,4 \
        vllm bench throughput --model meta-llama/Llama-4-Scout-17B-16E-Instruct \
          --tensor-parallel-size 2 --data-parallel-size 2 --enable-expert-parallel \
          --max-model-len 2048 --input-len 1000 --output-len 1000 --num-prompts 50  --trust-remote-code  --log-level DEBUG

  - label: "Llama TP only"
    gpu: h200
    num_gpus: 4
    commands:
      - CUDA_VISIBLE_DEVICES=1,2,3,4 \
        vllm bench throughput --model meta-llama/Llama-4-Scout-17B-16E-Instruct \
          --tensor-parallel-size 4 --data-parallel-size 1 --enable-expert-parallel \
          --max-model-len 2048 --input-len 1000 --output-len 1000 --num-prompts 50  --trust-remote-code  --log-level DEBUG

  - label: "Llama DP only"
    gpu: h200
    num_gpus: 4
    commands:
      - CUDA_VISIBLE_DEVICES=1,2,3,4 \
        vllm bench throughput --model meta-llama/Llama-4-Scout-17B-16E-Instruct \
          --tensor-parallel-size 1 --data-parallel-size 4 --enable-expert-parallel \
          --max-model-len 2048 --input-len 1000 --output-len 1000 --num-prompts 50  --trust-remote-code  --log-level DEBUG

  # - label: "Llama EP"
  #   gpu: h200
  #   num_gpus: 4
  #   commands:
  #     - CUDA_VISIBLE_DEVICES=1,2,3,4 \
  #       vllm bench throughput --model meta-llama/Llama-4-Scout-17B-16E-Instruct \
  #         --tensor-parallel-size 2 --data-parallel-size 2 --enable-expert-parallel \
  #         --max-model-len 2048 --input-len 1000 --output-len 1000 --num-prompts 50  --trust-remote-code

  # - label: "Llama EP: pplx + DeepGEMM"
  #   gpu: h200
  #   num_gpus: 4
  #   commands:
  #     - CUDA_VISIBLE_DEVICES=1,2,3,4 \
  #       VLLM_ALL2ALL_BACKEND=pplx VLLM_USE_DEEP_GEMM=1 \
  #       vllm bench throughput --model meta-llama/Llama-4-Scout-17B-16E-Instruct \
  #         --tensor-parallel-size 2 --data-parallel-size 2 --enable-expert-parallel \
  #         --max-model-len 2048 --input-len 1000 --output-len 1000 --num-prompts 50  --trust-remote-code

  # - label: "Llama EP: DeepEP + DeepGEMM"
  #   gpu: h200
  #   num_gpus: 4
  #   commands:
  #     - CUDA_VISIBLE_DEVICES=1,2,3,4 \
  #       VLLM_ALL2ALL_BACKEND=deepep_high_throughput VLLM_USE_DEEP_GEMM=1 \
  #       vllm bench throughput --model meta-llama/Llama-4-Scout-17B-16E-Instruct \
  #         --tensor-parallel-size 2 --data-parallel-size 2 --enable-expert-parallel \
  #         --max-model-len 2048 --input-len 1000 --output-len 1000 --num-prompts 50  --trust-remote-code

  # - label: "Llama EP: DeepEP"
  #   gpu: h200
  #   num_gpus: 4
  #   commands:
  #     - CUDA_VISIBLE_DEVICES=1,2,3,4 \
  #       VLLM_ALL2ALL_BACKEND=deepep_low_latency \
  #       vllm bench throughput --model meta-llama/Llama-4-Scout-17B-16E-Instruct \
  #         --tensor-parallel-size 2 --data-parallel-size 2 --enable-expert-parallel \
  #         --max-model-len 2048 --input-len 1000 --output-len 1000 --num-prompts 50  --trust-remote-code
