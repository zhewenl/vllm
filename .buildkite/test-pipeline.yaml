steps:
  - label: "DeepSeek"
    gpu: h200
    source_file_dependencies:
      - vllm/
    command: VLLM_ALL2ALL_BACKEND=pplx VLLM_USE_DEEP_GEMM=1 vllm serve deepseek-ai/DeepSeek-V3-0324 --tensor-parallel-size 1 --data-parallel-size 8 --enable-expert-parallel --max-model-len 1024


  - label: "Llama EP"
    gpu: h200
    num_gpus: 8
    commands:
      - vllm bench throughput --model meta-llama/Llama-4-Scout-17B-16E-Instruct --tensor-parallel-size 2 --data-parallel-size 4 --enable-expert-parallel --max-model-len 2048 --input-len 1000 --output-len 1000 --num-prompts 50

  - label: "Llama EP: pplx + DeepGEMM"
    gpu: h200
    num_gpus: 8
    commands:
      - VLLM_ALL2ALL_BACKEND=pplx VLLM_USE_DEEP_GEMM=1 vllm bench throughput --model meta-llama/Llama-4-Scout-17B-16E-Instruct --tensor-parallel-size 2 --data-parallel-size 4 --enable-expert-parallel  --max-model-len 2048 --input-len 1000 --output-len 1000 --num-prompts 50

  - label: "Llama EP: DeepEP + DeepGEMM"
    gpu: h200
    num_gpus: 8
    commands:
      - VLLM_ALL2ALL_BACKEND=deepep_high_throughput VLLM_USE_DEEP_GEMM=1 vllm bench throughput --model meta-llama/Llama-4-Scout-17B-16E-Instruct --tensor-parallel-size 2 --data-parallel-size 4 --enable-expert-parallel  --max-model-len 2048 --input-len 1000 --output-len 1000 --num-prompts 50

  - label: "Llama EP: DeepEP"
    gpu: h200
    num_gpus: 8
    commands:
      - VLLM_ALL2ALL_BACKEND=deepep_low_latency vllm bench throughput --model meta-llama/Llama-4-Scout-17B-16E-Instruct --tensor-parallel-size 2 --data-parallel-size 4 --enable-expert-parallel --max-model-len 2048 --input-len 1000 --output-len 1000 --num-prompts 50
